{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sample-level_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKeoOQvzqtbQ",
        "colab_type": "text"
      },
      "source": [
        "# **Spoken digit classifier using Sample-level CNN**\n",
        "\n",
        "Sample-level CNN classifies the spoken digit (0-9) on sample-level, without converting the signal into time-frequency level.\n",
        "\n",
        "\n",
        "*   The source of the dataset of mnist-speech is as follows:\n",
        "https://github.com/Jakobovski/free-spoken-digit-dataset\n",
        "(4 speakers, 2,000 recordings, 50 of each digit per speaker)\n",
        "*   The dataset is divided manually.\n",
        "(Train: 1,200, Valid: 400, Test: 400)\n",
        "*   The layer of CNN is constructed with reference to the following paper:\n",
        "https://arxiv.org/pdf/1703.01789.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJYdQZvy_44U",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "In this cell, speech-mnist dataset for deep learning is loaded.\n",
        "\n",
        "Also, python libraries for training is downloaded\n",
        "\n",
        "library list: tensorflow\n",
        "\n",
        "The path of dataset: /content/spoken_digital_classify/recordings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNecNnukzWuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/JiwonKKim/spoken_digit_classify.git\n",
        "!pip install tensorflow --upgrade"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuYQIZ8KB_40",
        "colab_type": "text"
      },
      "source": [
        "# Import the python libraries for training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NueqsqD_lcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "from scipy.io.wavfile import read\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWoIbSD0DYRz",
        "colab_type": "text"
      },
      "source": [
        "# Function to load dataset.\n",
        "\n",
        "The data is zero-padded after loaded.\n",
        "\n",
        "Padded data size per one wav file: 19683"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2euTbLhcDR-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_dataset(file_path, resample_size):\n",
        "    # list of wav file\n",
        "    file_list = os.listdir(file_path)\n",
        "\n",
        "    # shuffle the dataset\n",
        "    shuffle(file_list)\n",
        "    data, labels = [], []\n",
        "\n",
        "\n",
        "    for file_name in file_list:\n",
        "        _, wav_file = read(file_path + file_name)\n",
        "\n",
        "        # initialize the array which will be used as padded-data\n",
        "        padded = np.zeros(resample_size)\n",
        "\n",
        "        # Get the wav file length, and then insert it into the middle of zero-padded data\n",
        "        wav_file_len = len(wav_file)\n",
        "        padded_center = resample_size // 2\n",
        "        wav_center = wav_file_len // 2\n",
        "        front = padded_center - wav_center\n",
        "        padded[front:front + wav_file_len] = wav_file\n",
        "\n",
        "        # Append the data and the label as the dataset\n",
        "        data.append(padded)\n",
        "        labels.append(file_name[0])\n",
        "\n",
        "    data = np.array(data, dtype=float)\n",
        "    labels = np.array(labels, dtype=int)\n",
        "\n",
        "    total_data = len(data)\n",
        "    data = data.reshape(total_data, resample_size, 1)\n",
        "\n",
        "    return data, labels\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4UitTWFDxKw",
        "colab_type": "text"
      },
      "source": [
        "# Function of the layers\n",
        "\n",
        "The layer of CNN is constructed with reference to the following paper:\n",
        "https://arxiv.org/pdf/1703.01789.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtSlWP6gDqbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def top_layer(X_input, filters=128, kernel_size=3, strides=3, padding='same', activation='relu'):\n",
        "    X = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(X_input)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Activation(activation)(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def module_layer(X, filters=128, kernel_size=3, conv_strides=1,\n",
        "                 pool_size=3, pool_strides=3, padding='same', activation='relu'):\n",
        "    X = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=conv_strides, padding=padding)(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Activation(activation)(X)\n",
        "    X = tf.keras.layers.MaxPool1D(pool_size=pool_size, strides=pool_strides)(X)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "def bottom_layer(X, num_classes=10, filters=512, kernel_size=1, strides=1, padding='same',\n",
        "                 conv_active='relu', fc_active='sigmoid'):\n",
        "    X = tf.keras.layers.Conv1D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding)(X)\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Activation(conv_active)(X)\n",
        "    X = tf.keras.layers.Dropout(0.5)(X)\n",
        "    X = tf.keras.layers.Flatten()(X)\n",
        "    X = tf.keras.layers.Dense(num_classes, activation=fc_active)(X)\n",
        "\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXiQ2Pb0D7Kw",
        "colab_type": "text"
      },
      "source": [
        "# Loads dataset and sets hyperparameter\n",
        "\n",
        "The layer of CNN in this paper is called m^nDCNN. The input of this network should be the power of m, which refers to the filter length and pooling length of intermediate convolution layer, and n refers to the number of modules. This paper recommends to use the filter length as 3.\n",
        "Therefore, the longest data of this dataset is around 18000, I choose the length of zero-padded data (resample_size) as 19683, which is the power of 3.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkBuh6yuESmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# path direction\n",
        "train_path_dir = 'spoken_digit_classify/recordings/training/'\n",
        "valid_path_dir = 'spoken_digit_classify/recordings/validation/'\n",
        "test_path_dir = 'spoken_digit_classify/recordings/test/'\n",
        "\n",
        "# hyper-parameters\n",
        "resample_size = 19683\n",
        "num_classes = 10\n",
        "lr = 0.01\n",
        "input_shape = (resample_size, 1)\n",
        "batch_size = 100\n",
        "num_epochs = 50\n",
        "m = 3\n",
        "\n",
        "# divide the dataset into train, validation and test.\n",
        "train_data, train_labels = make_dataset(train_path_dir, resample_size)\n",
        "valid_data, valid_labels = make_dataset(valid_path_dir, resample_size)\n",
        "test_data, test_labels = make_dataset(test_path_dir, resample_size)\n",
        "\n",
        "train_labels_one_hot = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
        "valid_labels_one_hot = tf.keras.utils.to_categorical(valid_labels, num_classes)\n",
        "test_labels_one_hot = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
        "\n",
        "print('The number of train dataset: {}'.format(len(train_data)))\n",
        "print('The number of valid dataset: {}'.format(len(valid_data)))\n",
        "print('The number of test dataset: {}'.format(len(test_data)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB6jH1iTEosZ",
        "colab_type": "text"
      },
      "source": [
        "# SampleCNN layer\n",
        "Set the optimizer as SGD and print summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fy5B_GHEtWy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope(\"SampleCNN\"):\n",
        "    X_input = tf.keras.layers.Input(input_shape)\n",
        "\n",
        "    X = top_layer(X_input, filters=128, kernel_size=m, strides=m, padding='same', activation='relu')\n",
        "    # modules start\n",
        "    X = module_layer(X, filters=128, kernel_size=m, conv_strides=1,\n",
        "                     pool_size=m, pool_strides=m, padding='same', activation='relu')\n",
        "    X = module_layer(X, filters=128, kernel_size=m, conv_strides=1,\n",
        "                     pool_size=m, pool_strides=m, padding='same', activation='relu')\n",
        "    X = module_layer(X, filters=256, kernel_size=m, conv_strides=1,\n",
        "                     pool_size=m, pool_strides=m, padding='same', activation='relu')\n",
        "    X = module_layer(X, filters=256, kernel_size=m, conv_strides=1,\n",
        "                     pool_size=m, pool_strides=m, padding='same', activation='relu')\n",
        "    X = module_layer(X, filters=256, kernel_size=m, conv_strides=1,\n",
        "                     pool_size=m, pool_strides=m, padding='same', activation='relu')\n",
        "    X = module_layer(X, filters=256, kernel_size=m, conv_strides=1,\n",
        "                     pool_size=m, pool_strides=m, padding='same', activation='relu')\n",
        "    X = module_layer(X, filters=256, kernel_size=m, conv_strides=1,\n",
        "                     pool_size=m, pool_strides=m, padding='same', activation='relu')\n",
        "    X = module_layer(X, filters=512, kernel_size=m, conv_strides=1,\n",
        "                     pool_size=m, pool_strides=m, padding='same', activation='relu')\n",
        "    # modules end\n",
        "    X = bottom_layer(X, num_classes=num_classes, filters=512, kernel_size=1, strides=1, padding='same',\n",
        "                     conv_active='relu', fc_active='sigmoid')\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=X_input, outputs=X)\n",
        "\n",
        "sgd = tf.keras.optimizers.SGD(lr=lr, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "              optimizer=sgd,\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yg4eafYFFAZ",
        "colab_type": "text"
      },
      "source": [
        "# Starts training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKS-zZ94FMpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x=train_data, y=train_labels_one_hot, batch_size=batch_size, epochs=num_epochs,\n",
        "                    validation_data=(valid_data, valid_labels_one_hot))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGlzZM0XFfuW",
        "colab_type": "text"
      },
      "source": [
        "# Evaluates the training result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3bEqABtFfbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(x=test_data, y=test_labels_one_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfYo3HbYuGv5",
        "colab_type": "text"
      },
      "source": [
        "# Plot loss and accuracy of train and valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9yt2a-quITw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, loss_ax = plt.subplots()\n",
        "\n",
        "acc_ax = loss_ax.twinx()\n",
        "\n",
        "loss_ax.plot(history.history['loss'], 'y', label='train loss')\n",
        "loss_ax.plot(history.history['val_loss'], 'r', label='val loss')\n",
        "\n",
        "acc_ax.plot(history.history['accuracy'], 'b', label='train acc')\n",
        "acc_ax.plot(history.history['val_accuracy'], 'g', label='val acc')\n",
        "\n",
        "loss_ax.set_xlabel('epoch')\n",
        "loss_ax.set_ylabel('loss')\n",
        "acc_ax.set_ylabel('accuray')\n",
        "\n",
        "loss_ax.legend(loc='upper left')\n",
        "acc_ax.legend(loc='lower left')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}